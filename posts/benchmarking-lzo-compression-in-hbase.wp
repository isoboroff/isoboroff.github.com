<html><body><p>This is continued from my last post, <a href="http://pseudo.posterous.com/getting-clueweb-into-hbase">Getting Clueweb Into HBase</a>.  Comments from that post suggested trying LZO compression.  This required code from <a href="https://github.com/kevinweil/hadoop-lzo">Kevin Weil and Todd Lipcon</a> that implements LZO compression for Hadoop that works with CDH3b3, which is what I'm running.  I won't cover configuring LZO with Hadoop and HBase, since this is well documented in the documentation on the github site.</p><p></p> I created a new table, 'webtable2', with the additional option COMPRESSION =&gt; 'lzo' for the 'content' column family.  That is, the webpage content will be compressed, but the mapping from document identifiers to URLs is left uncompressed.  There certainly isn't any reason not to compress the 'meta' family too, but at this point I primarily wanted to test fetching pages out by URL and this is all in the 'content' table.<p></p> I reloaded all of ClueWeb09 into webtable2.  In contrast to my experience with the first load, loads took a consistent 3-4 hours per batch, which is probably attributable to having gone to 4GB regions, so a lot less regionsplits were taking place.  The result:<p></p> <span style="font-family: courier new,monospace;">$ hadoop fs -du /hbase</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">Found 8 items</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">3409            hdfs://node1:9000/hbase/-ROOT-</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">35602480        hdfs://node1:9000/hbase/.META.</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">0               hdfs://node1:9000/hbase/.corrupt</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">3542474         hdfs://node1:9000/hbase/.logs</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">0               hdfs://node1:9000/hbase/.oldlogs</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">3               hdfs://node1:9000/hbase/hbase.version</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">14955092829826  hdfs://node1:9000/hbase/webtable</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">2808802017826   hdfs://node1:9000/hbase/webtable2</span><p></p> The uncompressed webtable takes up 14.9TB on HDFS to store 12.5TB of text and about 50GB of URL-id mapping (not bad overhead at all).  The LZO version, however, only takes up 2.8TB.  Already a good reason to consider using compression.  Since I have all my HDFS blocks replicated three times, this is significant storage savings!<p></p> I then wrote a very simple benchmark, where a single client process makes requests with URLs and waits to receive each page:<p></p><span style="font-family: courier new,monospace;">public class Bench {</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">    public static void main(String args[]) throws Exception {</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">    if (args.length != 2) {</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">        System.out.println("Usage: Bench [table] [inputfile]");</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">        System.exit(-1);</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">    }</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">    </span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">    Configuration config = HBaseConfiguration.create();</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">    HTable table = new HTable(config, args[0]);</span><p></p><span style="font-family: courier new,monospace;">    BufferedReader in = new BufferedReader(new FileReader(args[1]));</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">    String query = null;</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">    long c_count = 0;</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">    long m_count = 0;</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">    long start = System.currentTimeMillis();</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">    long last = start;</span><p></p><span style="font-family: courier new,monospace;">    while ((query = in.readLine()) != null) {</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">        try {</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">        if (query.startsWith("http://")) {</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">            query = Util.reverse_hostname(query);</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">        } else if (query.startsWith("clueweb")) {</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">            Get g = new Get(Bytes.toBytes(query));</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">            Result r = table.get(g);</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">            byte[] value = r.getValue(Bytes.toBytes("meta"),</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">                          Bytes.toBytes("url"));</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">            query = Bytes.toString(value);</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">            m_count++;</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">        }</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">        </span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">        Get g = new Get(Bytes.toBytes(query));</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">        Result r = table.get(g);</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">        byte[] value = r.getValue(Bytes.toBytes("content"), </span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">                      Bytes.toBytes("raw"));</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">        c_count++;</span><p></p><span style="font-family: courier new,monospace;">        if ((c_count % 10000) == 0) {</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">            long now = System.currentTimeMillis();</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">            double sec = (now - last) / 1000.0;</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">            double rate = sec / 10000.0;</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">            System.out.println("("+c_count+") 10,000 queries in " +</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">                       sec + "s (" + rate + " s/q)");</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">            last = System.currentTimeMillis();</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">        }</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">                       </span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">        } catch (IOException e) {</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">            continue;</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">        }</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">    }</span><p></p> <span style="font-family: courier new,monospace;">    long end = System.currentTimeMillis();</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">    System.out.println("Fetched " + c_count + " content records.");</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">    if (m_count &gt; 0) {</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">        System.out.println("Fetched " + m_count + " meta records.");</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">    }</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">    System.out.println("Total time: " + (end - start) / 1000.0 + "s");</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">    System.out.println("Time per fetch: " </span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">               + ((end - start) / ((c_count + m_count) * 1000.0))</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">               + "s");</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">    }</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">}</span><p></p>This class reads URLs from a file to fetch.  The benchmark isn't measuring peak query response rate, since to do that it would make more sense to have lots of clients asking simultaneously.  However, it does measure a reasonable rate of requests to see if response times are sufficient and that we don't leak memory or anything else.  (My usage scenario only has perhaps a few dozen users.)<p></p> I took a sample of 500,000 URLs from the collection to use for testing.  Before running, I shut down HBase (master and region server processes) and started them up again.  (I neglected to shut down HDFS as well, perhaps I should have.)  First, running against the uncompressed table and printing timing information after every 10k requests:<p></p> <span style="font-family: courier new,monospace;">(10000) 10,000 queries in 2936.995s (0.2936995 s/q)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">(20000) 10,000 queries in 2723.424s (0.2723424 s/q)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">(30000) 10,000 queries in 1781.611s (0.17816110000000002 s/q)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">(40000) 10,000 queries in 774.718s (0.0774718 s/q)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">(50000) 10,000 queries in 844.356s (0.0844356 s/q)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">(60000) 10,000 queries in 1300.254s (0.13002539999999999 s/q)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">(70000) 10,000 queries in 1276.55s (0.127655 s/q)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">(80000) 10,000 queries in 1261.369s (0.1261369 </span><span style="font-family: courier new,monospace;">s/q</span><span style="font-family: courier new,monospace;">)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">(90000) 10,000 queries in 1207.213s (0.12072129999999999 </span><span style="font-family: courier new,monospace;">s/q</span><span style="font-family: courier new,monospace;">)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">(100000) 10,000 queries in 1206.334s (0.1206334 </span><span style="font-family: courier new,monospace;">s/q</span><span style="font-family: courier new,monospace;">)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">(110000) 10,000 queries in 1404.424s (0.1404424 </span><span style="font-family: courier new,monospace;">s/q</span><span style="font-family: courier new,monospace;">)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">(120000) 10,000 queries in 1209.425s (0.1209425 </span><span style="font-family: courier new,monospace;">s/q</span><span style="font-family: courier new,monospace;">)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">(130000) 10,000 queries in 1137.359s (0.11373589999999999 </span><span style="font-family: courier new,monospace;">s/q</span><span style="font-family: courier new,monospace;">)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">(140000) 10,000 queries in 800.359s (0.08003590000000001 </span><span style="font-family: courier new,monospace;">s/q</span><span style="font-family: courier new,monospace;">)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">(150000) 10,000 queries in 943.502s (0.0943502 </span><span style="font-family: courier new,monospace;">s/q</span><span style="font-family: courier new,monospace;">)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">(160000) 10,000 queries in 724.696s (0.07246960000000001 </span><span style="font-family: courier new,monospace;">s/q</span><span style="font-family: courier new,monospace;">)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">(170000) 10,000 queries in 975.958s (0.0975958 </span><span style="font-family: courier new,monospace;">s/q</span><span style="font-family: courier new,monospace;">)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">...<p></p></span>The times seem to hover around 1/10s per query, which is perfectly reasonable for my usage scenario.  (If it wasn't, the thing to do is to spread the regions across more regionservers.  There is also a request bottleneck at the .META. table which is an HBase limitation.)  There is also a lot of variation in time between batches, with the shortest average 0.077s/q and the longest 0.29s/q.  It doesn't get shorter consistently through the run, so caching is not helping beyond a certain point... there are simply too many regions to cache efficiently.  System load on the cluster nodes was not an issue at all.<p></p> Now, here is timing of the same sequence of fetches (on a clean startup of HBase) against the compressed webtable:<p></p><span style="font-family: courier new,monospace;">(10000) 10,000 queries in 561.779s (0.0561779 </span><span style="font-family: courier new,monospace;">s/q</span><span style="font-family: courier new,monospace;">)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">(20000) 10,000 queries in 484.254s (0.0484254 </span><span style="font-family: courier new,monospace;">s/q</span><span style="font-family: courier new,monospace;">)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">(30000) 10,000 queries in 512.418s (0.051241800000000004 </span><span style="font-family: courier new,monospace;">s/q</span><span style="font-family: courier new,monospace;">)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">(40000) 10,000 queries in 514.108s (0.05141079999999999 </span><span style="font-family: courier new,monospace;">s/q</span><span style="font-family: courier new,monospace;">)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">(50000) 10,000 queries in 515.848s (0.05158479999999999 </span><span style="font-family: courier new,monospace;">s/q</span><span style="font-family: courier new,monospace;">)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">(60000) 10,000 queries in 513.325s (0.0513325 </span><span style="font-family: courier new,monospace;">s/q</span><span style="font-family: courier new,monospace;">)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">(70000) 10,000 queries in 456.037s (0.0456037 </span><span style="font-family: courier new,monospace;">s/q</span><span style="font-family: courier new,monospace;">)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">(80000) 10,000 queries in 472.251s (0.0472251 </span><span style="font-family: courier new,monospace;">s/q</span><span style="font-family: courier new,monospace;">)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">(90000) 10,000 queries in 459.83s (0.045982999999999996 </span><span style="font-family: courier new,monospace;">s/q</span><span style="font-family: courier new,monospace;">)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">(100000) 10,000 queries in 488.919s (0.048891899999999995 </span><span style="font-family: courier new,monospace;">s/q</span><span style="font-family: courier new,monospace;">)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">(110000) 10,000 queries in 485.988s (0.0485988 </span><span style="font-family: courier new,monospace;">s/q</span><span style="font-family: courier new,monospace;">)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">(120000) 10,000 queries in 468.895s (0.0468895 </span><span style="font-family: courier new,monospace;">s/q</span><span style="font-family: courier new,monospace;">)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">(130000) 10,000 queries in 485.07s (0.048507 </span><span style="font-family: courier new,monospace;">s/q</span><span style="font-family: courier new,monospace;">)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">(140000) 10,000 queries in 488.118s (0.0488118 </span><span style="font-family: courier new,monospace;">s/q</span><span style="font-family: courier new,monospace;">)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">(150000) 10,000 queries in 480.102s (0.048010199999999996 </span><span style="font-family: courier new,monospace;">s/q</span><span style="font-family: courier new,monospace;">)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">(160000) 10,000 queries in 461.39s (0.046139 </span><span style="font-family: courier new,monospace;">s/q</span><span style="font-family: courier new,monospace;">)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">(170000) 10,000 queries in 509.506s (0.0509506 </span><span style="font-family: courier new,monospace;">s/q</span><span style="font-family: courier new,monospace;">)</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">...<p></p></span>With compression, query batches average twice as fast to complete, and also those times are much more consistent.  Compressing takes 20% of the space in HDFS (before replication) and provides much faster query response times.  Win!</body></html>