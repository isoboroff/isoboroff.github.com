<p><html><body><p>SIGIR 2010 (<a href="http://sigir2010.org/">http://sigir2010.org/</a>) accepted 87 papers out of 520, giving a 16.7% acceptance rate. ??Is SIGIR a competitive conference? ??Is it too competitive?</p><p></p>Acceptance rate is a terrible number. ??The numerator (87) is not really free to adjust, because it is dictated by the structure of the conference, by adding sessions or tracks, or changing talk durations. ??For a conference like SIGIR with a long history, these are hard decisions to make. ??The denominator is also uncontrollable, because anyone can submit a paper; submission doesn't imply acceptable-ness. ??Thus, the acceptance rate is not useful in showing how selective a conference is, nor can you tell if the acceptance rate is good or bad, nor could a program committee do anything to change it if it wanted to.<p></p> #include&lt;disclaimers.h&gt;... I submitted a paper (with Ben Carterette) which was accepted. ??I'm a member of the 2010 senior program committee and have been either on that or the general program committee for a long time.<p></p> To my mind, the important question isn't what was the rate of acceptance over all papers, but what was the rate of acceptance over acceptable papers. ??How many reasonable papers were rejected?<p></p>SIGIR uses ConfMaster, and for the overall recommendation there is a 6-point scale. ??In SIGIR PC meetings I've been involved in, there are three sets of papers: clear accepts, clear rejects, and a bunch in the middle for which SPC members argue for or against. ??(This is clearly not a good system, because we depend on two SPC members to bring up either uninteresting papers which were clear accepts, or low-scoring papers who might have been misreviewed, but it is the system that exists. ??Interested parties should join <a href="http://groups.google.com/group/sigir-meta">http://groups.google.com/group/sigir-meta</a>.) ??In general, papers are labeled accept on the basis of review scores, metareviews, and SPC discussion, by a process in which scores are a component but do not explain the full outcome. ??A straightforward competitive-acceptance-rate would compute the accept rate in the first two groups.<p></p> Let's suppose that of the 520 papers submitted to SIGIR 2010, 50 were deemed clear accepts by score average (including SPC score), and 300 were deemed clear rejects by score average. ??(In this discussion, I am making up numbers that resemble the data I have but are fudged in the interests of preserving the privacy of the SPC review process). ??170 papers are left in the middle. ??Since 87 papers were accepted, this makes the competitive-acceptance-rate 51%.<p></p> An improvement to the above process would separate SPC scores from PC scores, and drop low-confidence reviews.<p></p>Another refinement would attempt to drill down into that group of 170 discussion candidates, and also include papers in the bottom tier of 300 which were brought up for discussion. ??In some sense, this "discussion range" of papers represents the wiggle room in the acceptance rate. ??The definite-accept pile places a hard but not impenetrable ceiling on the number of accepts that can come from here -- in this case, 37 papers -- but certainly a definite-accept should be demoted in favor of a compelling discussion-range paper. ??This is hard to accomplish in SIGIR because of structural and cultural barriers, but let's assume the possibility exists. ??37/170 = a 21.7% discussion range acceptance rate, with perhaps minor changes from bottom-tier discussions and top-tier demotions. ??The discussion-range-acceptance-rate is a better measure of competitiveness because it isn't as controlled by the number of submissions.<p></p> I'll throw this open for discussion at this point, you will doubtless have better answers than I do.</body></html></p>