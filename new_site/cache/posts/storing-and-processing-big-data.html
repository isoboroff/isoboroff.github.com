<p><html><body><p>I was asked, as part of a symposium at the Library of Congress, to speak briefly on the subject of requirements for storing and processing big data, including social media and web datasets.  This post is meant to accompany that talk as a list of references and links.  Some of the thinking here is influenced by <a href="http://www.dataists.com/2010/09/a-taxonomy-of-data-science/">Dataists inaugural post on the topic</a> by Hilary Mason and Chris Wiggins, which itself reminds me a lot of Kernighan and Plauger's book <a href="http://www.amazon.com/Software-Tools-Brian-W-Kernighan/dp/020103669X">Software Tools</a>. </p><p></p> (Disclaimer: in this post I will mention many software products and some companies.  These mentions do not imply any endorsement of those companies or products, but are meant to be illustrative of the state of the art and common usage in the field.)<p></p> What is "big data", anyway?  The generations of yore (like, 20 years ago) measured it in gigabytes.  In 1991, <a href="http://trec.nist.gov/">TREC</a> brought the information retrieval community (the researchers who studied what would come to be called "search engines") from working with megabytes to a two gigabyte text corpus.  This caused tremendous engineering havoc in the research world; we might assume that industrial groups of the time already worked with data an order of magnitude or more larger.  Today's largest web collection made generally available to researchers (at cost! thanks to NSF), <a href="http://boston.lti.cs.cmu.edu/Data/clueweb09/">CLuEWeb09</a>, is 25 terabytes of raw web pages, approximately equivalent to the top tier of a commercial web search engine.  Social media datasets, such as the <a href="http://blogs.loc.gov/loc/2010/04/how-tweet-it-is-library-acquires-entire-twitter-archive/">Twitter archive to be housed at LOC</a>, exist on different scales; the text in such a collection might be a hundreds of gigabytes or a few terabytes, but occupy a graph structure of billions of nodes.<p></p> So the answer is, "big data" is data that's bigger than what you can comfortably store and process right now.  As a comparative, if you've got it, it probably isn't big anymore ("My data is bigger than your data.")  Or, perhaps more realistically, "You thought that was big, wait to see what's coming down the pipe!"<p></p> I won't divide up into sections on storage and processing, because they depend on each other... if you store things in a relational database, that implies certain kinds of processing; if you first think of streaming over data, that implies certain kinds of storage.  My focus is also on what can be done with commodity hardware or cloud resources... I'll try to be up-front about infrastructure costs when I can.  I'm also cheap, so I tend to favor free solutions; the good news in 2010 is that free is as good or better than anything you can pay for, and the next best thing is pretty darn cheap.<p></p> You'd be alarmed at what a modern desktop computer can handle.  A terabyte of hard disk in 2010 costs about $90, plus another $20 if you want it to fit in your laptop.  Two-core processors are now previous-generation; four and eight cores on the desktop are more and more common.  2 GB of RAM is too small for Windows these days, so 4 and 8GB is getting more common.  You can run Windows, Linux, or Mac OS on essentially equivalent hardware, so you can pick the platform you like best.  My experience is in the Unix world, so my perspective is going to center primarily on Linux, Mac OS, or Windows with Cygwin, but that's not a functional requirement of anything here.  This roughly plain-vanilla desktop computer will cost you around $1500-2000 and can handle a terabyte or three of data.<p></p> "Handle" in the context of this article means that you can process the data, slice, dice, and explore it, probably not at interactive speeds, but that you're willing to wait a few minutes or an hour for results.  In the commercial world, the issue is scaling up to serving results in real-time while the data keeps flooding in... I'm not going there right now.<p></p> In the Unix world, the best tools come for free; see the aforementioned Software Tools book for philosophical perspective (but not necessarily freedom).  For the neophyte I recommend <a href="http://www.amazon.com/Think-UNIX-Jon-Lasser/dp/078972376X/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1285523360&amp;sr=1-1">Think Unix</a> by Jon Lasser.  Using basic Unix tools, you can<ul><li>reformat your data (sed, awk, perl, tr, and lots more)</li><li>search your data (grep and others)</li><li>cut it into fields (cut, awk)</li><li>transform it in any number of ways (you name it!)</li><li>count occurrences (uniq, nl, grep)</li><li>sort data (sort)</li></ul><p>and do all kinds of exploratory data analysis.  These tools work on plain text files, which is a great simple file format.  They work in a stream fashion, so you can pipe them together to make complex operations.  (My favorite is using "sort | uniq -c | sort -n" to count repeat occurrences, then produce something like a histogram.)  On modern Unixes (i.e., anything you're likely to encounter now), those big text files get cached as you read them and so you can actually scream through data pretty quickly.  This works great for exploring data and trying out ideas without writing or buying a single piece of software, but just using the screwdrivers and wrenches that came with the operating system.</p><p></p> As an example of this usage, I help run a large evaluation in experimental web search.  As part of this, folks send me large ranked lists of documents from the CLuEWeb collection, and I show those web pages to people who decide what's relevant and what's not.  When I get the lists, the documents are keyed by an identifier, but I like to show my users the URL as well since that can be a helpful criterion.  I have a master list of all 1 billion URLs in CLuEWeb with their identifiers, a 50GB text file.  Using just the tools I mentioned above, I can join the identifier lists against the master URL list, and produce my URL lists for my users to review, in a few minutes on my desktop Mac.<p></p> Many of you would point out that this is an obvious job for a relational database, and I might even agree except I'm not such a database person.  However, I could put both the master URL list and the ranked lists I receive into a database and get much the same result from a simple query or two.  Databases seem alarming at first but in reality they're a simple way to store and process data, provided you have a good idea about how to store it from the beginning.  The good news is that you can make bad layout decisions and the database will humor you until you have simply too much data.  As with Unix tools, there are very good free tools, such as <a href="http://www.mysql.com/">MySQL</a> and <a href="http://www.postgresql.org/">PostgreSQL.</a>  There are also expensive commercial solutions, but again, remember the goal here is to explore big data and be a researcher, not to field a commercial realtime solution.  MySQL and PostgreSQL will run just fine on your standard desktop machine and won't cost you a dime.<p></p> Another common idiom for processing big data is to use scripting languages like <a href="http://www.python.org/">Python</a> or <a href="http://www.perl.org/">Perl</a>.  Scripting languages are called that because they didn't used to be compiled; nowadays everything is compiled just-in-time.  The key point is that these languages support rapidly turning a good, complicated idea into a short, fast, reusable computer program.<br> If writing computer programs drove you nuts 10 years ago, come back and give Python a try.  Again, these tools are free and run on any operating system.<p></p>Since I started out saying that if you can hold it in your hand, it isn't big data anymore, I will spend some time talking about how to scale up a storage and processing infrastructure.  Databases do this well for certain kinds of data, but this can be complicated and costly.  The hot idea nowadays is "cloud computing".  Without getting into a buzzword battle, let's call a bunch of equivalent, interchangeable, and anonymous computers working together a cloud.  (Just like a weather cloud is a bunch of equivalent, interchangeable, anonymous droplets of water vapor, working together).  These days you can make your own cloud or rent one.<p></p> I built my own little cloud last year, so I can talk about how this is done.  Each individual computer costs around $1500 and has 8 cores and 8GB of memory.  It also has 8 slots for disks; 8 1.5TB disks costs around $800, so the total cost per computer is $2300.  In cloud parlance, each computer is called a "node".  There is also a certain amount of infrastructure in the form of a computer rack, a network switch, and two uninterruptible power supplies.  A single rack holds 15 nodes, or 15 * 8 * 1.5 = 180 TB of raw disk.<p></p> The key point of cloud computing, in the context of this article, is to keep your data close to the computer doing the work on it.  If your agency or company has a central file store, you are probably aware that your computer has to copy files from the server before you can work on them.  In our little cloud, the idea is that each CPU core is going to work on the data that is sitting on the disks on that machine, and avoid copying things as much as possible.  This way, all the cores can work at the same time without waiting for each other.  This is why my design above has one disk per core per GB of RAM.  (A better system would have 2-4GB of RAM per core; I'm waiting for RAM prices to fall.)<p></p> The common software for computing on a cloud like this is <a href="http://hadoop.apache.org/">Hadoop</a>.  Hadoop includes two key components: a storage infrastucture for making all those disks look like a single disk, and making it reliable; and a programming paradigm called <a href="http://en.wikipedia.org/wiki/MapReduce">Map-Reduce</a>.  Map-Reduce is a style of programming that maximizes letting all those cores work on a part of the problem, then merging the results together easily.  In the Map step, each core gets a piece of the data and can transform it into one or more other pieces of data.  After the Map step, all the data on all the machines gets sorted in parallel so that the data is in order.  The Reduce step, the sorted data are merged together.  This paradigm is extremely flexible and a lot of problems are easy to structure this way.<p></p> One example is indexing a large web crawl.  If you want to search the web, you need to note down all the words in all the web pages, and put them into an index, so that when someone gives you a query with a word, finding the pages that contain that word is fast.  To think about this problem in Map Reduce, imagine that we have a series of web pages, 1 to 1 billion, and each page is a URL and its content.  We'll write this as (URL, content); geeks call that a tuple.  In the Map step, we take each (URL, content) tuple, cut the content into words, and output a series of tuples (word, URL) for each word in the page.  After mapping, these tuples get sorted by word.  The Reduce step collects all the (word, URL) tuples for a single word and outputs the index fragment (word, (URL, URL, ...)), all the URLs for that word.  These get stored someplace where we can easily find them by word.  Voila, search engine.  Well, almost.  The key point is that Map-Reduce automates the splitting up the work and the sorting across a cloud, and gives you a way to think about how to break down problems efficiently in that modality.<p></p> Lots of <a href="http://en.wikipedia.org/wiki/Graph_theory">graph problems</a>, like those that come up in social media, break down the same way.  Graphs in this context are collections of "nodes" and "edges", where edges connect two or more nodes.  Nodes might be people, and edges might be friendship relations.  Edges are tuples in the Map Reduce framework.  Sometimes, a special graph database is called for.  This can help for storing a large graph structure once and automating certain kinds of processing on the graph.  A good graph database, again free, is <a href="http://neo4j.org/">Neo4J</a>.<p></p> A nice Windows tool for graph analytics is <a href="http://nodexl.codeplex.com/">NodeXL</a>.  It runs in Excel and allows exploration of small-to-medium datasets, well, as large as Excel might let you scale.  Its author, Marc Smith, is a prominent figure in social network study.<p></p> The Hadoop ecosystem includes lots of tools for thinking about big data problems as <a href="http://hadoop.apache.org/common/docs/r0.15.2/streaming.html">streams</a>, <a href="http://hbase.apache.org/">databases</a>, <a href="http://hadoop.apache.org/pig/">flow networks</a>, and more.  My favorite tool for statistical analysis is <a href="http://www.r-project.org/">R</a>.  Folks have been working on extending it to Hadoop; I would appreciate some good links on this as I'm not too familiar with this yet.  <p></p> Renting a cloud in many cases makes more sense than building one.  Amazon's <a href="http://aws.amazon.com/elasticmapreduce/">Elastic Map-Reduce</a> allows you to essentially borrow computers at Amazon.com, link them together in a Map Reduce cloud, run your experiment, and store all your data on their cloud-based storage service.  <a href="http://www.rackspace.com/index.php">RackSpace</a> is a company which does managed hosting and will essentially rent you cloud.  Other companies exist; again, the above are not an endorsement but just examples of what people use.<p></p> <span style="text-decoration: underline;">Further reading</span><p></p>Blogs: (others happily included, pls comment)<ul><li><a href="http://www.dataists.com/">Dataists</a> (Hillary Mason of <a href="http://bit.ly">bit.ly</a> and others)</li><li><a href="http://datamining.typepad.com/data_mining/">Text Mining, Visualization, and Social Media</a> (Matt Hurst of Microsoft)</li><li><a href="http://www.cloudera.com/">Cloudera Hadoop and Big Data</a>; Cloudera provides a Hadoop distribution and some glossy-brochure style blog posts</li></ul><p>Academic conferences: (lots of conference are concerned with big data these days... the following are primarily focused on web and social media.  Again, comments appreciated)</p><ul><li><a href="http://icwsm.org/">AAAI International Conference on Weblogs and Social Media (ICWSM)</a>, a venue blending social science, social media, and data science; they also provide some social media datasets for free.</li><li><a href="http://www.wsdm-conference.org/">ACM International Conference on Web Search and Data Mining (WSDM)</a>, a web-oriented big data forum.</li></ul></body></html></p>